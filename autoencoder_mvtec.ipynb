{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = '/content/drive/MyDrive/ACV/mvtec_anomaly_detection'\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, base_dir, data_type, transform=None):\n",
        "        self.base_dir = base_dir\n",
        "        self.data_type = data_type\n",
        "        self.transform = transform\n",
        "        self.image_files = []\n",
        "        self.targets = []\n",
        "        self.normal_count = 0\n",
        "        self.anomaly_count = 0\n",
        "\n",
        "        for category in os.listdir(base_dir):\n",
        "            category_path = os.path.join(base_dir, category)\n",
        "            if os.path.isdir(category_path):\n",
        "                type_path = os.path.join(category_path, data_type)\n",
        "                if os.path.isdir(type_path):\n",
        "                    for defect_class in os.listdir(type_path):\n",
        "                        defect_class_path = os.path.join(type_path, defect_class)\n",
        "                        if os.path.isdir(defect_class_path):\n",
        "                            for img_file in os.listdir(defect_class_path):\n",
        "                                img_file_path = os.path.join(defect_class_path, img_file)\n",
        "                                if img_file.lower().endswith(('png', 'jpg', 'jpeg')):\n",
        "                                    self.image_files.append(img_file_path)\n",
        "                                    if data_type == 'train' or defect_class.lower() == 'good':\n",
        "                                        self.targets.append(0)  # Normal images\n",
        "                                        self.normal_count += 1\n",
        "                                    else:\n",
        "                                        self.targets.append(1)  # Anomalous images\n",
        "                                        self.anomaly_count += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.image_files[index]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        label = self.targets[index]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "img_transform = T.Compose([\n",
        "    T.Resize((64, 64)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "\n",
        "train_data = MyDataset(data_dir, 'train', transform=img_transform)\n",
        "test_data = MyDataset(data_dir, 'test', transform=img_transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=64, drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=False, batch_size=64, drop_last=True)\n",
        "\n",
        "print(f\"Training images: {len(train_data)}\")\n",
        "print(f\"  Normal in training set: {train_data.normal_count}\")\n",
        "print(f\"Testing images: {len(test_data)}\")\n",
        "print(f\"  Normal in test set: {test_data.normal_count}\")\n",
        "print(f\"  Anomalous in test set: {test_data.anomaly_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "learning_rate = 1e-4\n",
        "training_steps = 30  # Vary for more epochs if needed\n",
        "batch_sz = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Training on {'GPU' if torch.cuda.is_available() else 'CPU'}.\")\n",
        "\n",
        "# Model\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "autoencoder = AutoEncoder().to(device)\n",
        "autoencoder_optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
        "print(f'> Model parameters: {sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main training loop\n",
        "train_losses = []\n",
        "for step in tqdm(range(training_steps)):\n",
        "    x, _ = next(iter(train_loader))\n",
        "    x = x.to(device)\n",
        "\n",
        "\n",
        "    outputs = autoencoder(x)\n",
        "    loss = nn.MSELoss()(outputs, x)\n",
        "\n",
        "    autoencoder_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    autoencoder_optimizer.step()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step: {step}   Loss: {loss.item():.3f}\")\n",
        "\n",
        "        reconstructions = torchvision.utils.make_grid(torch.clamp(outputs, 0, 1).detach(), padding=0, nrow=8)\n",
        "        true_images = torchvision.utils.make_grid(torch.clamp(x, 0, 1).detach(), padding=0, nrow=8)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.title('True Images')\n",
        "        plt.imshow(np.transpose(true_images.cpu().numpy(), (1, 2, 0)))\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.title('Reconstructed Images')\n",
        "        plt.imshow(np.transpose(reconstructions.cpu().numpy(), (1, 2, 0)))\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dynamic_threshold(loss_vals):\n",
        "    mean_loss = np.mean(loss_vals)\n",
        "    std_loss = np.std(loss_vals)\n",
        "    return mean_loss + 2 * std_loss \n",
        "\n",
        "initial_threshold = dynamic_threshold(train_losses)\n",
        "print(f\"Initial anomaly detection threshold: {initial_threshold}\")\n",
        "\n",
        "def reconstruction_errors(autoencoder, data_loader):\n",
        "    autoencoder.eval()\n",
        "    errors = []\n",
        "    true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            imgs = imgs.to(device)\n",
        "            outputs = autoencoder(imgs)\n",
        "            loss = torch.mean((outputs - imgs) ** 2, dim=[1, 2, 3])\n",
        "            errors.extend(loss.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "    return np.array(errors), np.array(true_labels)\n",
        "\n",
        "recon_errors, true_lbls = reconstruction_errors(autoencoder, test_loader)\n",
        "\n",
        "\n",
        "assert recon_errors.shape == true_lbls.shape, \"Shape mismatch between errors and true labels.\"\n",
        "\n",
        "\n",
        "anomalies_detected = recon_errors > initial_threshold\n",
        "print(f\"Detected anomalies: {np.sum(anomalies_detected)}\")\n",
        "\n",
        "true_labels_list = []\n",
        "pred_labels_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc=\"Anomaly Detection\"):\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = autoencoder(imgs)\n",
        "        recon_error = torch.mean((outputs - imgs) ** 2, dim=[1, 2, 3]).cpu().numpy()\n",
        "        true_labels_list.extend(labels.cpu().numpy())\n",
        "        pred_labels_list.extend([1 if e > initial_threshold else 0 for e in recon_error])\n",
        "\n",
        "print(f\"Anomalies detected in the test set: {np.sum(anomalies_detected)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Error distribution plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title('Reconstruction Error Distribution')\n",
        "sns.histplot(recon_errors, bins=100, kde=True, color='blue')\n",
        "plt.xlabel('Reconstruction Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = autoencoder(imgs)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title('Reconstructed Images')\n",
        "plt.imshow(np.transpose(torchvision.utils.make_grid(outputs.cpu().detach(), padding=2, normalize=True), (1, 2, 0)))\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# training loss\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.title('Training Loss')\n",
        "plt.plot(train_losses)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Loss distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title('Loss Distribution')\n",
        "sns.histplot(recon_errors, bins=100, kde=True, color='blue')\n",
        "plt.axvline(initial_threshold, color='r')\n",
        "plt.axvline(0.0, color='b')\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels_list, pred_labels_list)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title('Confusion Matrix for Anomaly Detection')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
